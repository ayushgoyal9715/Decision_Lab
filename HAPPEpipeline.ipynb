{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "814f52ed-bbc4-4954-90b7-8b5ff30ac10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "from mne.time_frequency import tfr_multitaper\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd80569f-9471-478b-a6b2-65436b797b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chan_IDs = ['FP1', 'FP2' , 'F7' ,'F8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "083d98f5-210b-4291-8a5d-1fc11778381b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_visualizations_semiautomated = True\n",
    "vis_freq_min = 2\n",
    "vis_freq_max = 57\n",
    "freq_to_plot = [6, 10, 20, 30, 55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "58a0975a-9a23-4dc7-96c8-7c52dd221ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_visualizations_semiautomated = True\n",
    "vis_freq_min = 2\n",
    "vis_freq_max = 57\n",
    "freq_to_plot = [6, 10, 20, 30, 55]\n",
    "task_EEG_processing = False\n",
    "segment_data = True\n",
    "segment_length = 2 \n",
    "segment_interpolation = True\n",
    "segment_rejection = False\n",
    "average_rereference = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "94e60e4f-fc2d-4cb8-8860-f7580ae587e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_folder_name='eeg_data'\n",
    "file_names = [f for f in os.listdir(src_folder_name) if f.endswith('.edf')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "67713172-809e-4622-b053-4291ceb80a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {\n",
    "    \"Number_ICs_Rejected\": [],\n",
    "    \"File_Length_In_Secs\": [],\n",
    "    \"Percent_Variance_Kept_of_Post_Waveleted_Data\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "75bdd9ba-2b41-44d3-8fd6-127c3e040279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # Switch to the TkAgg backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "993d18b0-c0e1-4bcb-ba79-188426869362",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_and_extract_intervals(file_path):\n",
    "    # Load the raw data\n",
    "    raw = mne.io.read_raw_edf(file_path, preload=True)\n",
    "\n",
    "    montage_channels = ['T4', 'A2', 'C3', 'C4', 'A1', 'T3', 'O1', 'T5', 'O2', \n",
    "                    'F8', 'T6', 'FZ', 'FP2', 'F7', 'FP1', 'CZ']\n",
    "    \n",
    "    raw.pick_channels(montage_channels)\n",
    "    \n",
    "    channel_positions = np.array([\n",
    "    [5.18e-15, -84.5, -8.85],  # T4\n",
    "    [3.68e-15, -60.1, -60.1],  # A2\n",
    "    [3.87e-15, 63.2, 56.9],    # C3\n",
    "    [-3.87e-15, -63.2, 56.9],  # C4\n",
    "    [3.68e-15, 60.1, -60.1],   # A1\n",
    "    [5.18e-15, 84.5, -8.85],   # T3\n",
    "    [-80.8, 26.1, -4],         # O1\n",
    "    [-49.9, 68.4, -7.49],      # T5\n",
    "    [-80.8, -26.1, -4],        # O2\n",
    "    [49.9, -68.4, -7.49],      # F8\n",
    "    [-49.9, -68.4, -7.49],     # T6\n",
    "    [60.7, 0, 59.5],           # FZ\n",
    "    [80.8, -26.1, -4],         # FP2\n",
    "    [49.9, 68.4, -7.49],       # F7\n",
    "    [80.8, 26.1, -4],          # FP1\n",
    "    [5.2e-15, 0, 85],          # CZ\n",
    "    [57.6, 48.2, 39.9],        # F3\n",
    "    [57.6, -48.1, 39.9],       # F4\n",
    "    [32.9, 0, 78.4]            # FCz\n",
    "    ])\n",
    "    available_channels = [ch for ch in montage_channels if ch in raw.info['ch_names']]\n",
    "\n",
    "    # Adjust positions array to match available channels\n",
    "    positions_dict = dict(zip(available_channels, channel_positions))\n",
    "    positions_dict = {ch: positions_dict[ch] for ch in available_channels if ch in positions_dict}\n",
    "    channel_positions = np.array([positions_dict[ch] for ch in available_channels])\n",
    "\n",
    "    # Create the montage with the updated channel positions\n",
    "    montage = mne.channels.make_dig_montage(\n",
    "        ch_pos=dict(zip(available_channels, channel_positions)), \n",
    "        coord_frame='head'\n",
    "    )\n",
    "\n",
    "    raw = raw.pick_channels(chan_IDs)\n",
    "\n",
    "    \n",
    "    # Filter the data (1 Hz highpass, 50 Hz notch for powerline noise)\n",
    "    raw.filter(1, 50, fir_design='firwin')\n",
    "\n",
    "    # Plot the power spectrum if visualization is enabled\n",
    "    if pipeline_visualizations_semiautomated:\n",
    "        raw.plot_psd(fmin=vis_freq_min, fmax=vis_freq_max)\n",
    "\n",
    "    # Re-reference data to average if specified\n",
    "    if average_rereference:\n",
    "        raw.set_eeg_reference('average', projection=True)\n",
    "\n",
    "    # # Re-reference data to the specified channel\n",
    "    # reference_channel='FCz'\n",
    "    # if reference_channel in raw.info['ch_names']:\n",
    "    #     raw.set_eeg_reference(ref_channels=[reference_channel], projection=True)\n",
    "    # else:\n",
    "    #     raise ValueError(f\"The reference channel '{reference_channel}' is not present in the data.\")\n",
    "    \n",
    "\n",
    "    # Segmentation (epoching the data)\n",
    "    if segment_data:\n",
    "        if not task_EEG_processing:\n",
    "            events = mne.make_fixed_length_events(raw, duration=segment_length)\n",
    "        else:\n",
    "            # If task-related EEG, use predefined conditions (not applicable here)\n",
    "            raise NotImplementedError(\"Task EEG not implemented yet.\")\n",
    "\n",
    "        # Epoch the data\n",
    "        # Set the baseline to a reasonable range (e.g., 200 ms before the event to the event itself)\n",
    "        # epochs = mne.Epochs(raw, events, tmin=-0.2, tmax=segment_length, baseline=(None, 0), preload=True)\n",
    "        sfreq = raw.info['sfreq']\n",
    "        max_time = raw.times[-1]\n",
    "        interval_1=(300, 600)\n",
    "        interval_2=(1500, 1860)\n",
    "            \n",
    "        # Convert intervals to seconds\n",
    "        interval_1_sec = (interval_1[0] / sfreq, interval_1[1] / sfreq)\n",
    "        interval_2_sec = (interval_2[0] / sfreq, interval_2[1] / sfreq)\n",
    "            \n",
    "        # Adjust intervals if they exceed the recording duration\n",
    "        interval_1_sec = (max(0, interval_1_sec[0]), min(max_time, interval_1_sec[1]))\n",
    "        interval_2_sec = (max(0, interval_2_sec[0]), min(max_time, interval_2_sec[1]))\n",
    "\n",
    "        no_stress_interval = raw.copy().crop(tmin=interval_1_sec[0], tmax=interval_1_sec[1])\n",
    "        stressed_interval = raw.copy().crop(tmin=interval_2_sec[0], tmax=interval_2_sec[1])\n",
    "        epochs_no_stress=mne.make_fixed_length_epochs(no_stress_interval,duration=1,overlap=0.5)\n",
    "        array_no_streesed=epochs_no_stress.get_data()\n",
    "                \n",
    "        epochs_stressed=mne.make_fixed_length_epochs(stressed_interval,duration=1,overlap=0.5)\n",
    "        array_streesed=epochs_stressed.get_data()\n",
    "\n",
    "    \n",
    "    # Record metrics for each file (example)\n",
    "    metrics[\"File_Length_In_Secs\"].append(raw.times[-1])\n",
    "    return array_no_streesed,array_streesed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d515507d-e1ee-430b-b177-4c09029ef97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /Users/ashishupadhyay/Desktop/E Club Secy/Decison Lab/eeg_data/0_20170726_102501.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 356599  =      0.000 ...  1426.396 secs...\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-pass filter from 1 - 50 Hz\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandpass filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower passband edge: 1.00\n",
      "- Lower transition bandwidth: 1.00 Hz (-6 dB cutoff frequency: 0.50 Hz)\n",
      "- Upper passband edge: 50.00 Hz\n",
      "- Upper transition bandwidth: 12.50 Hz (-6 dB cutoff frequency: 56.25 Hz)\n",
      "- Filter length: 825 samples (3.300 s)\n",
      "\n",
      "NOTE: plot_psd() is a legacy function. New code should use .compute_psd().plot().\n",
      "Effective window size : 8.192 (s)\n",
      "Plotting power spectral density (dB=True).\n",
      "EEG channel type selected for re-referencing\n",
      "Adding average EEG reference projection.\n",
      "1 projection items deactivated\n",
      "Average reference projection was added, but has not been applied yet. Use the apply_proj method to apply it.\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "Using data from preloaded Raw for 1 events and 250 original time points ...\n",
      "0 bad epochs dropped\n",
      "Not setting metadata\n",
      "1 matching events found\n",
      "No baseline correction applied\n",
      "Created an SSP operator (subspace dimension = 1)\n",
      "1 projection items activated\n",
      "Using data from preloaded Raw for 1 events and 250 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/g5/b9cy28ks44x_v0t6r5r41ptw0000gn/T/ipykernel_95365/893470896.py:52: FutureWarning: The value of `amplitude='auto'` will be removed in MNE 1.8.0, and the new default will be `amplitude=False`.\n",
      "  raw.plot_psd(fmin=vis_freq_min, fmax=vis_freq_max)\n",
      "/var/folders/g5/b9cy28ks44x_v0t6r5r41ptw0000gn/T/ipykernel_95365/893470896.py:52: RuntimeWarning: Channel locations not available. Disabling spatial colors.\n",
      "  raw.plot_psd(fmin=vis_freq_min, fmax=vis_freq_max)\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to hold data and labels\n",
    "all_data = []\n",
    "all_labels = []\n",
    "\n",
    "for file in file_names:\n",
    "    file_path=os.path.join(src_folder_name, file)\n",
    "    no_stressed,stressed = load_and_extract_intervals(file_path)    \n",
    "    all_data.append(stressed[0])\n",
    "    all_data.append(no_stressed[0])\n",
    "    all_labels.append(1)\n",
    "    all_labels.append(0)\n",
    "\n",
    "all_data = np.array(all_data)\n",
    "all_labels = np.array(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "92bb06cf-f550-43d8-bb6f-2b0e382739c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "all_data = scaler.fit_transform(all_data.reshape(-1, all_data.shape[-1])).reshape(all_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd5af5d8-c563-42b6-86ba-512789878946",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(all_data.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "all_data = all_data[indices]\n",
    "all_labels = all_labels[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddeee540-df13-4d6c-a89c-661b770aa7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(all_data, all_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31f02c1a-5cb5-45aa-8366-f419fee83f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "946a1d03-74be-4b9c-9855-e5f76750decb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    def __init__(self, num_classes=2, chans=4, samples=128):\n",
    "        super(EEGNet, self).__init__()\n",
    "\n",
    "        # First Conv2D layer (Temporal Convolutions)\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1, 64), padding=(0, 32), bias=False)\n",
    "        self.batchnorm1 = nn.BatchNorm2d(16)\n",
    "\n",
    "        # Depthwise Convolution (Spatio-Temporal filtering)\n",
    "        self.depthwiseConv = nn.Conv2d(16, 32, kernel_size=(chans, 1), groups=16, bias=False)\n",
    "        self.batchnorm2 = nn.BatchNorm2d(32)\n",
    "        self.elu = nn.ELU()\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=(1, 4))\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "\n",
    "        # Separable Convolutions\n",
    "        self.separableConv = nn.Conv2d(32, 32, kernel_size=(1, 16), padding=(0, 8), bias=False)\n",
    "        self.batchnorm3 = nn.BatchNorm2d(32)\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=(1, 8))\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "\n",
    "        # Fully Connected Layer\n",
    "        self.fc1 = nn.Linear(32 * (samples // 32), num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First convolutional block (Temporal)\n",
    "        x = self.conv1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.depthwiseConv(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Separable convolutions\n",
    "        x = self.separableConv(x)\n",
    "        x = self.batchnorm3(x)\n",
    "        x = self.elu(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Flatten and fully connected layer\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Define model, loss function, and optimizer\n",
    "model = EEGNet(num_classes=2, chans=4, samples=X_train.shape[2])  # Update with your actual input shape\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84502b97-7d64-416e-99aa-9fb306548e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EEGNet(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(1, 64), stride=(1, 1), padding=(0, 32), bias=False)\n",
       "  (batchnorm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (depthwiseConv): Conv2d(16, 32, kernel_size=(4, 1), stride=(1, 1), groups=16, bias=False)\n",
       "  (batchnorm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (elu): ELU(alpha=1.0)\n",
       "  (pool1): AvgPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0)\n",
       "  (dropout1): Dropout(p=0.25, inplace=False)\n",
       "  (separableConv): Conv2d(32, 32, kernel_size=(1, 16), stride=(1, 1), padding=(0, 8), bias=False)\n",
       "  (batchnorm3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool2): AvgPool2d(kernel_size=(1, 8), stride=(1, 8), padding=0)\n",
       "  (dropout2): Dropout(p=0.25, inplace=False)\n",
       "  (fc1): Linear(in_features=224, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ccc9b82f-38f3-4410-935b-da1504767df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20  # Number of epochs to train for\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs.unsqueeze(1))  # Add a channel dimension\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
    "    \n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f82a9289-4198-4f99-8ebc-40ae4e1a35ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize counters for correct predictions and total samples\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Disable gradient computation during testing\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        # Move inputs and labels to the appropriate device (CPU or GPU)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Add a channel dimension to the inputs if necessary\n",
    "        outputs = model(inputs.unsqueeze(1))  # Add the channel dimension\n",
    "\n",
    "        # Get the predicted class (the index of the max log-probability)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        # Update the total number of samples\n",
    "        total += labels.size(0)\n",
    "        \n",
    "        # Update the number of correct predictions\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        # Optionally print the predicted and true labels for inspection\n",
    "        print(f\"Predicted: {predicted.cpu().numpy()}\")\n",
    "        print(f\"Labels: {labels.cpu().numpy()}\")\n",
    "\n",
    "# Calculate and print the accuracy as a percentage\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy: {accuracy}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
